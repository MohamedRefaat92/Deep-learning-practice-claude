{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Learning Practice Sessions for Genomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum for Bioinformaticians\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Learning Resources\n- **Book 1**: Deep Learning with PyTorch (Second Edition) - Manning MEAP\n- **Book 2**: Learn Generative AI with PyTorch - Mark Liu\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Practice Session Overview\n\nThis curriculum progresses from foundational PyTorch skills to advanced genomics applications, with each session building upon previous concepts.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEVEL 1: PyTorch Fundamentals**\n\n### **Session 1: Tensor Basics and Operations**\n**Source**: Deep Learning with PyTorch, Chapter 3 - \"It starts with a tensor\"\n\n**Genomics Context**: Representing DNA sequences and gene expression data\n\n**Learning Objectives:**\n- Create and manipulate PyTorch tensors\n- Understand tensor shapes, indexing, and slicing\n- Perform basic operations on genomic data representations\n\n**Practice Exercises:**\n\n1. **DNA Sequence Encoding**\n   - Create a one-hot encoding function for DNA sequences (A, T, G, C)\n   - Convert a batch of DNA k-mers (length 6) into tensor format\n   - Practice tensor reshaping and concatenation\n\n2. **Gene Expression Matrix**\n   - Load a simulated gene expression matrix (genes \u00d7 samples)\n   - Perform normalization using tensor operations\n   - Calculate summary statistics (mean, std) across dimensions\n\n3. **Sequence Manipulation**\n   - Implement reverse complement using tensor operations\n   - Create sliding windows over sequence tensors\n   - Practice broadcasting with position weight matrices\n\n**Code Starter:**\n```python\nimport torch\n\n# Exercise 1: DNA encoding\ndef one_hot_encode_sequence(sequence):\n    # TODO: Implement one-hot encoding for DNA\n    pass\n\n# Exercise 2: Expression data\nexpression_data = torch.randn(1000, 50)  # 1000 genes, 50 samples\n# TODO: Normalize and analyze\n\n# Exercise 3: Sequence operations\ndna_seq = \"ATCGATCG\"\n# TODO: Implement reverse complement\n```\n\n---\n\n### **Session 2: Autograd and Gradient Descent**\n**Source**: Deep Learning with PyTorch, Chapter 5 - \"The mechanics of learning\"\n\n**Genomics Context**: Optimizing predictive models for genomic features\n\n**Learning Objectives:**\n- Understand automatic differentiation\n- Implement simple gradient descent\n- Apply to basic genomics prediction tasks\n\n**Practice Exercises:**\n\n1. **Linear Model for Expression Prediction**\n   - Build a linear model to predict gene expression from regulatory features\n   - Manually implement gradient descent\n   - Compare with PyTorch autograd\n\n2. **Loss Function Exploration**\n   - Implement MSE loss for regression tasks\n   - Implement cross-entropy for variant classification\n   - Visualize loss landscapes\n\n3. **Learning Rate Optimization**\n   - Experiment with different learning rates\n   - Implement learning rate scheduling\n   - Apply to a simple genomics dataset\n\n**Code Starter:**\n```python\nimport torch\n\n# Exercise 1: Manual gradient descent\ndef predict_expression(features, weights, bias):\n    return features @ weights + bias\n\n# TODO: Implement training loop with manual gradients\n\n# Exercise 2: With autograd\nweights = torch.randn(10, 1, requires_grad=True)\nbias = torch.randn(1, requires_grad=True)\n# TODO: Implement using autograd\n```\n\n---\n\n### **Session 3: Building Neural Networks**\n**Source**: Deep Learning with PyTorch, Chapter 6 - \"Using a neural network to fit the data\"\n\n**Genomics Context**: Creating models for sequence classification\n\n**Learning Objectives:**\n- Use nn.Module for model building\n- Implement forward pass\n- Understand layer types and activations\n\n**Practice Exercises:**\n\n1. **Promoter Classifier**\n   - Build a 3-layer MLP to classify promoter vs non-promoter sequences\n   - Use appropriate activation functions\n   - Implement the model using nn.Module\n\n2. **Gene Expression Regressor**\n   - Create a neural network to predict gene expression from histone marks\n   - Experiment with different architectures\n   - Add dropout for regularization\n\n3. **Multi-task Model**\n   - Build a model with shared layers and multiple output heads\n   - Predict both expression level and tissue type\n   - Implement custom loss weighting\n\n**Code Starter:**\n```python\nimport torch\nimport torch.nn as nn\n\nclass PromoterClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        # TODO: Define layers\n        \n    def forward(self, x):\n        # TODO: Implement forward pass\n        pass\n\n# Exercise: Instantiate and test\nmodel = PromoterClassifier(input_size=4*200, hidden_size=128)\n```\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEVEL 2: Deep Learning Architectures**\n\n### **Session 4: Convolutional Neural Networks**\n**Source**: Deep Learning with PyTorch, Chapter 8 - \"Using convolutions to generalize\"\n\n**Genomics Context**: Motif detection and sequence analysis\n\n**Learning Objectives:**\n- Understand 1D convolutions for sequences\n- Implement CNNs for genomic data\n- Use pooling and multiple convolutional layers\n\n**Practice Exercises:**\n\n1. **Motif Detector**\n   - Build a 1D CNN to detect transcription factor binding motifs\n   - Visualize learned filters\n   - Interpret what the CNN has learned\n\n2. **Splice Site Predictor**\n   - Create a CNN to predict splice sites from sequence context\n   - Use multiple convolutional layers with different kernel sizes\n   - Implement global pooling\n\n3. **DeepBind-style Model**\n   - Replicate the architecture from DeepBind paper\n   - Train on simulated protein-DNA binding data\n   - Extract position weight matrices from filters\n\n**Code Starter:**\n```python\nimport torch.nn as nn\n\nclass MotifCNN(nn.Module):\n    def __init__(self, seq_length, num_filters=32, kernel_size=12):\n        super().__init__()\n        # TODO: Implement 1D CNN architecture\n        self.conv1 = nn.Conv1d(4, num_filters, kernel_size)\n        # Add more layers\n        \n    def forward(self, x):\n        # x shape: (batch, 4, seq_length)\n        # TODO: Implement forward pass\n        pass\n```\n\n---\n\n### **Session 5: Recurrent Neural Networks**\n**Source**: Deep Learning with PyTorch, Chapter 9 - \"Using PyTorch to fight cancer\"\n\n**Genomics Context**: Sequential data processing for genomics\n\n**Learning Objectives:**\n- Implement RNNs, LSTMs, and GRUs\n- Handle variable-length sequences\n- Apply to sequential genomic data\n\n**Practice Exercises:**\n\n1. **Variable-Length Sequence Classifier**\n   - Build an LSTM to classify sequences of varying lengths\n   - Implement sequence padding and packing\n   - Handle batch processing efficiently\n\n2. **Protein Sequence Property Predictor**\n   - Use bidirectional LSTM for protein secondary structure prediction\n   - Implement many-to-many architecture\n   - Handle amino acid sequences\n\n3. **Regulatory Element Scanner**\n   - Create a GRU-based model to scan for regulatory elements\n   - Implement attention mechanism\n   - Visualize attention weights on sequences\n\n**Code Starter:**\n```python\nimport torch.nn as nn\n\nclass SequenceLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        # TODO: Add output layers\n        \n    def forward(self, x, lengths):\n        # TODO: Implement with packing/unpacking\n        pass\n```\n\n---\n\n### **Session 6: Transfer Learning**\n**Source**: Deep Learning with PyTorch, Chapter 10 - \"Combining data sources\"\n\n**Genomics Context**: Using pre-trained models and fine-tuning\n\n**Learning Objectives:**\n- Understand transfer learning concepts\n- Fine-tune pre-trained models\n- Adapt models to new genomic tasks\n\n**Practice Exercises:**\n\n1. **Fine-tune a Pre-trained CNN**\n   - Start with a model trained on general sequence classification\n   - Fine-tune for specific organism or tissue type\n   - Implement freezing and unfreezing layers\n\n2. **Domain Adaptation**\n   - Train on mouse genomic data\n   - Adapt to human genomic data with limited samples\n   - Use appropriate loss functions\n\n3. **Multi-organism Model**\n   - Create a model that handles multiple species\n   - Implement species-specific branches\n   - Share common sequence processing layers\n\n**Code Starter:**\n```python\n# Load pre-trained model\npretrained_model = torch.load('sequence_model.pt')\n\n# Freeze early layers\nfor param in pretrained_model.conv_layers.parameters():\n    param.requires_grad = False\n\n# TODO: Add new classification head\n# TODO: Implement fine-tuning strategy\n```\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEVEL 3: Advanced Topics**\n\n### **Session 7: Attention Mechanisms**\n**Source**: Deep Learning with PyTorch, Advanced chapters\n\n**Genomics Context**: Interpretable sequence models\n\n**Learning Objectives:**\n- Implement self-attention mechanisms\n- Build transformer-style models\n- Apply to genomic sequences\n\n**Practice Exercises:**\n\n1. **Sequence Attention Model**\n   - Implement multi-head attention for DNA sequences\n   - Visualize attention patterns\n   - Identify important positions\n\n2. **Cross-Attention for Sequence Pairs**\n   - Build a model for RNA-protein interaction prediction\n   - Use cross-attention between sequences\n   - Implement position encoding\n\n3. **Mini-Transformer**\n   - Create a simplified transformer for sequence classification\n   - Implement multiple attention layers\n   - Compare with CNN and RNN approaches\n\n**Code Starter:**\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        # TODO: Implement multi-head attention\n        \n    def forward(self, query, key, value, mask=None):\n        # TODO: Implement attention mechanism\n        pass\n\nclass SequenceTransformer(nn.Module):\n    # TODO: Build transformer model\n    pass\n```\n\n---\n\n### **Session 8: Variational Autoencoders (VAE)**\n**Source**: Learn Generative AI with PyTorch, Early chapters on VAEs\n\n**Genomics Context**: Dimensionality reduction and data generation\n\n**Learning Objectives:**\n- Understand VAE architecture\n- Implement reparameterization trick\n- Generate synthetic genomic data\n\n**Practice Exercises:**\n\n1. **Gene Expression VAE**\n   - Build a VAE for gene expression data\n   - Learn latent representations\n   - Visualize latent space\n\n2. **Sequence Generation VAE**\n   - Create a VAE that generates DNA sequences\n   - Ensure biological validity\n   - Sample from latent space\n\n3. **Conditional VAE**\n   - Implement a conditional VAE for cell-type-specific generation\n   - Control generation with labels\n   - Interpolate between cell types\n\n**Code Starter:**\n```python\nimport torch\nimport torch.nn as nn\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super().__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            # TODO: Define encoder layers\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            # TODO: Define decoder layers\n        )\n        \n    def encode(self, x):\n        # TODO: Implement encoding\n        pass\n        \n    def reparameterize(self, mu, logvar):\n        # TODO: Implement reparameterization trick\n        pass\n        \n    def decode(self, z):\n        # TODO: Implement decoding\n        pass\n        \n    def forward(self, x):\n        # TODO: Implement full forward pass\n        pass\n\ndef vae_loss(recon_x, x, mu, logvar):\n    # TODO: Implement VAE loss (reconstruction + KL divergence)\n    pass\n```\n\n---\n\n### **Session 9: Generative Adversarial Networks (GANs)**\n**Source**: Learn Generative AI with PyTorch, Chapters on GANs\n\n**Genomics Context**: Generating realistic genomic sequences\n\n**Learning Objectives:**\n- Understand GAN training dynamics\n- Implement generator and discriminator\n- Apply to genomic data generation\n\n**Practice Exercises:**\n\n1. **Sequence GAN**\n   - Build a GAN to generate DNA sequences\n   - Ensure mode coverage\n   - Evaluate quality of generated sequences\n\n2. **Expression Profile GAN**\n   - Generate realistic gene expression profiles\n   - Condition on cell type or treatment\n   - Implement techniques to stabilize training\n\n3. **Wasserstein GAN**\n   - Implement WGAN with gradient penalty\n   - Apply to generating protein sequences\n   - Compare with standard GAN\n\n**Code Starter:**\n```python\nimport torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super().__init__()\n        # TODO: Define generator architecture\n        \n    def forward(self, z):\n        # TODO: Generate samples from noise\n        pass\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        # TODO: Define discriminator architecture\n        \n    def forward(self, x):\n        # TODO: Classify real vs fake\n        pass\n\n# Training loop\ndef train_gan(generator, discriminator, dataloader, epochs):\n    # TODO: Implement alternating training\n    pass\n```\n\n---\n\n### **Session 10: Diffusion Models**\n**Source**: Learn Generative AI with PyTorch, Advanced chapters\n\n**Genomics Context**: State-of-the-art sequence generation\n\n**Learning Objectives:**\n- Understand diffusion process\n- Implement forward and reverse diffusion\n- Generate high-quality genomic sequences\n\n**Practice Exercises:**\n\n1. **Simple Diffusion Model**\n   - Implement DDPM for 1D sequence data\n   - Understand noise scheduling\n   - Generate DNA sequences\n\n2. **Conditional Diffusion**\n   - Build a classifier-free guided diffusion model\n   - Condition on genomic features\n   - Control generation with guidance scale\n\n3. **Protein Design with Diffusion**\n   - Create a diffusion model for protein sequence generation\n   - Incorporate structural constraints\n   - Sample novel sequences with desired properties\n\n**Code Starter:**\n```python\nimport torch\nimport torch.nn as nn\n\nclass DiffusionModel(nn.Module):\n    def __init__(self, seq_length, hidden_dim):\n        super().__init__()\n        # TODO: Define the noise prediction network\n        \n    def forward(self, x, t):\n        # TODO: Predict noise at timestep t\n        pass\n\nclass DiffusionProcess:\n    def __init__(self, num_timesteps=1000):\n        self.num_timesteps = num_timesteps\n        # TODO: Define beta schedule\n        \n    def q_sample(self, x_0, t, noise):\n        # Forward diffusion process\n        # TODO: Add noise according to schedule\n        pass\n        \n    def p_sample(self, model, x_t, t):\n        # Reverse diffusion step\n        # TODO: Implement denoising step\n        pass\n        \n    def sample(self, model, shape):\n        # TODO: Generate samples\n        pass\n```\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEVEL 4: Production & Optimization**\n\n### **Session 11: Model Optimization**\n**Source**: Deep Learning with PyTorch, Chapter 12 - \"Using deployable models\"\n\n**Learning Objectives:**\n- Optimize model performance\n- Reduce memory footprint\n- Improve inference speed\n\n**Practice Exercises:**\n\n1. **Mixed Precision Training**\n   - Implement automatic mixed precision for a genomics model\n   - Compare training speed and memory usage\n   - Ensure numerical stability\n\n2. **Model Quantization**\n   - Quantize a trained sequence classifier\n   - Evaluate accuracy vs size tradeoffs\n   - Deploy quantized model\n\n3. **Pruning and Distillation**\n   - Prune a large model for mobile deployment\n   - Distill knowledge to a smaller student model\n   - Measure inference time improvements\n\n**Code Starter:**\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Mixed precision training\nscaler = GradScaler()\n\ndef train_step(model, data, target):\n    with autocast():\n        output = model(data)\n        loss = criterion(output, target)\n    \n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n```\n\n---\n\n### **Session 12: Large-Scale Genomics**\n**Source**: Both books, scaling concepts\n\n**Genomics Context**: Working with whole-genome datasets\n\n**Learning Objectives:**\n- Handle large genomic datasets\n- Implement efficient data loading\n- Distribute training across GPUs\n\n**Practice Exercises:**\n\n1. **Efficient Data Pipeline**\n   - Create a custom Dataset for large genomic files\n   - Implement efficient data loading with DataLoader\n   - Use memory mapping for huge files\n\n2. **Multi-GPU Training**\n   - Implement DataParallel for sequence models\n   - Use DistributedDataParallel for better performance\n   - Handle batch normalization correctly\n\n3. **Genomic Data Augmentation**\n   - Implement sequence augmentation techniques\n   - Random reverse complement and mutations\n   - Create augmentation pipeline\n\n**Code Starter:**\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GenomicDataset(Dataset):\n    def __init__(self, fasta_file, labels_file):\n        # TODO: Implement efficient loading\n        pass\n        \n    def __len__(self):\n        # TODO\n        pass\n        \n    def __getitem__(self, idx):\n        # TODO: Return sequence and label\n        pass\n\n# Multi-GPU training\nmodel = nn.DataParallel(model)\n# or\nfrom torch.nn.parallel import DistributedDataParallel\nmodel = DistributedDataParallel(model)\n```\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PROJECT-BASED SESSIONS**\n\n### **Capstone Project 1: Variant Effect Prediction**\n**Combines**: Sessions 1-6\n\n**Objective**: Build an end-to-end model to predict the functional impact of genetic variants\n\n**Components:**\n1. Data preprocessing and encoding\n2. CNN + attention architecture\n3. Training with transfer learning\n4. Model interpretation and visualization\n5. Performance evaluation on held-out test set\n\n---\n\n### **Capstone Project 2: Single-Cell RNA-seq Analysis**\n**Combines**: Sessions 7-9\n\n**Objective**: Create a generative model for single-cell gene expression\n\n**Components:**\n1. VAE for dimensionality reduction\n2. Clustering in latent space\n3. Conditional generation of cell types\n4. Trajectory analysis\n5. Novel cell state generation\n\n---\n\n### **Capstone Project 3: De Novo Sequence Design**\n**Combines**: Sessions 8-10\n\n**Objective**: Generate novel functional genomic sequences\n\n**Components:**\n1. Train diffusion model on functional sequences\n2. Implement conditional generation\n3. Validate generated sequences\n4. Optimize for desired properties\n5. Compare with real sequences\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Assessment Guidelines\n\nFor each session:\n- **Understanding**: Can you explain the concepts in your own words?\n- **Implementation**: Can you write the code without looking at solutions?\n- **Adaptation**: Can you modify the code for new genomics problems?\n- **Debugging**: Can you fix errors and understand why they occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Learning Tips\n\n1. **Code First**: Type out all examples, don't copy-paste\n2. **Experiment**: Change hyperparameters and observe effects\n3. **Visualize**: Plot losses, weights, and outputs\n4. **Document**: Add comments explaining genomics context\n5. **Validate**: Use biological knowledge to check if results make sense\n6. **Iterate**: Start simple, then add complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Additional Resources\n\n- PyTorch Documentation: https://pytorch.org/docs/\n- Genomics Deep Learning Papers: AlphaFold, DeepBind, Basenji, Enformer\n- Practice Datasets: UCI ML Repository, ENCODE, GTEx\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Getting Started\n\nStart with **Session 1** and complete all exercises before moving forward. Each session should take 2-4 hours to complete thoroughly. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}