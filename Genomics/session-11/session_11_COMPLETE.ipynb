{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 11: Model Optimization and Deployment\n",
        "**Book Ref**: Deep Learning PyTorch Ch. 12, 15 (pages 371-415) | **Duration**: 3-4 hours\n",
        "\n",
        "## Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.amp.autocast()  # Mixed precision\n",
        "torch.quantization.quantize_dynamic()  # Quantization\n",
        "torch.jit.script()  # JIT compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Mixed Precision Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Model Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Compare sizes\n",
        "original_size = os.path.getsize('model.pth') / 1e6\n",
        "quantized_size = os.path.getsize('quantized_model.pth') / 1e6\n",
        "print(f\"Size reduction: {original_size:.1f}MB -> {quantized_size:.1f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Model Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Prune 30% of weights in linear layers\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# Make pruning permanent\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        prune.remove(module, 'weight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*ONNX export and deployment included*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}