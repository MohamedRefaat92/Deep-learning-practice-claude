# Session 3: Building Neural Networks with nn.Module

## Overview
This session teaches you how to build custom neural network models using PyTorch's `nn.Module` class.

## Files
- `session_03_neural_networks_COMPLETE.md` - Complete session with all exercises

## Topics Covered
- Subclassing nn.Module
- Building multi-layer perceptrons (MLPs)
- Activation functions (ReLU, Sigmoid, Tanh)
- Dropout regularization
- Batch normalization
- Weight initialization
- Promoter sequence classification
- Multi-task learning

## Duration
3-4 hours

## Prerequisites
- Session 1: Tensor Basics (completed)
- Session 2: Autograd & Gradient Descent (completed)

## Exercises
1. Basic MLP for gene expression
2. Promoter sequence classifier
3. Activation function comparison
4. Batch normalization effects
5. Dropout regularization
6. Weight initialization
7. Multi-task learning (challenge)

## Book References
- Deep Learning with PyTorch Ch. 6 (pages 175-220)
- Learn Generative AI with PyTorch Ch. 2, Appendix B

## How to Use
The markdown file contains all code and explanations. You can:
1. Read through in any markdown viewer
2. Copy code blocks into Jupyter cells
3. Run code in Google Colab
4. Use as reference while coding

## Next Steps
After completing this session, you'll be ready for:
- Session 4: CNNs for Sequences
- Session 5: RNNs for Sequences

Request generation with: "Please generate Session 4 in full detail"
