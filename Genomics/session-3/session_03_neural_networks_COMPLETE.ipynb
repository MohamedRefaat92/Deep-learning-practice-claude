{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 3: Building Neural Networks with nn.Module\n",
        "## Complete Guide to Deep Learning Architectures for Genomics\n",
        "\n",
        "**\ud83d\udcd6 Book References**:\n",
        "- Deep Learning with PyTorch Ch. 6 (pages 175-220)\n",
        "- Learn Generative AI with PyTorch Ch. 2, Appendix B\n",
        "\n",
        "**\u23f1\ufe0f Duration**: 3-4 hours  \n",
        "**\ud83c\udfaf Difficulty**: Beginner-Intermediate\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83c\udfaf Learning Objectives & Core Functions\n",
        "\n",
        "### What You'll Learn:\n",
        "\u2705 Build custom models with `nn.Module`  \n",
        "\u2705 Implement forward passes  \n",
        "\u2705 Use activation functions effectively  \n",
        "\u2705 Apply regularization (dropout, batch norm)  \n",
        "\u2705 Build genomics classifiers  \n",
        "\u2705 Initialize weights properly\n",
        "\n",
        "### Core PyTorch Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn.Module              # Base class\n",
        "nn.Linear()            # Fully connected layer  \n",
        "nn.ReLU()              # Activation\n",
        "nn.Dropout()           # Regularization\n",
        "nn.BatchNorm1d()       # Normalization\n",
        "nn.Sequential()        # Container\n",
        "model.parameters()     # Get parameters\n",
        "model.train()          # Training mode\n",
        "model.eval()           # Evaluation mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udcda Quick Theory\n",
        "\n",
        "### The nn.Module Pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()        # MUST call this!\n",
        "        self.layer1 = nn.Linear(10, 20)\n",
        "        self.layer2 = nn.Linear(20, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Points**:\n",
        "- `__init__`: Define layers\n",
        "- `forward`: Define computation\n",
        "- Don't call `forward()` directly, use `model(x)`\n",
        "- `super().__init__()` is REQUIRED\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 1: Basic Neural Network\n",
        "\n",
        "### Task: Build 2-layer network for gene expression prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)\n",
        "n_samples = 200\n",
        "tf_binding = torch.rand(n_samples, 3)  # 3 TF binding scores\n",
        "gene_expression = (tf_binding[:, 0] * 2 + \n",
        "                   tf_binding[:, 1] * -1 + \n",
        "                   tf_binding[:, 2] * 0.5 + \n",
        "                   torch.randn(n_samples) * 0.1)\n",
        "\n",
        "print(f\"Features shape: {tf_binding.shape}\")\n",
        "print(f\"Labels shape: {gene_expression.shape}\")\n",
        "\n",
        "# Simple MLP\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create and inspect model\n",
        "model = SimpleMLP(input_size=3, hidden_size=10, output_size=1)\n",
        "print(f\"\\\\nModel:\\\\n{model}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\\\nTotal parameters: {total_params}\")\n",
        "\n",
        "# Training function\n",
        "def train(model, X, y, epochs=100, lr=0.01):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        # Forward\n",
        "        y_pred = model(X).squeeze()\n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Train\n",
        "losses = train(model, tf_binding, gene_expression, epochs=100)\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(tf_binding).squeeze()\n",
        "    mse = F.mse_loss(predictions, gene_expression)\n",
        "    print(f\"\\\\nFinal MSE: {mse.item():.4f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(gene_expression, predictions, alpha=0.5)\n",
        "plt.plot([gene_expression.min(), gene_expression.max()], \n",
        "         [gene_expression.min(), gene_expression.max()], \n",
        "         'r--', label='Perfect Prediction')\n",
        "plt.xlabel('True Expression')\n",
        "plt.ylabel('Predicted Expression')\n",
        "plt.title('Predictions vs True Values')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 2: Promoter Sequence Classifier\n",
        "\n",
        "### Task: Classify DNA sequences as promoters or non-promoters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_sequence(seq):\n",
        "    \\\"\\\"\\\"One-hot encode DNA sequence.\\\"\\\"\\\"\n",
        "    mapping = {'A': [1,0,0,0], 'C': [0,1,0,0], \n",
        "               'G': [0,0,1,0], 'T': [0,0,0,1]}\n",
        "    encoded = [mapping.get(base, [0,0,0,0]) for base in seq.upper()]\n",
        "    return torch.tensor(encoded, dtype=torch.float32).flatten()\n",
        "\n",
        "# Generate synthetic promoter data\n",
        "def generate_promoter_data(n_samples=500):\n",
        "    promoters = []\n",
        "    non_promoters = []\n",
        "    \n",
        "    # Promoter motif: TATA box\n",
        "    for _ in range(n_samples // 2):\n",
        "        # Promoters have TATA box\n",
        "        seq = 'TATAAAA' + ''.join(np.random.choice(['A','C','G','T'], 43))\n",
        "        promoters.append((seq, 1))\n",
        "        \n",
        "        # Non-promoters don't\n",
        "        seq = ''.join(np.random.choice(['A','C','G','T'], 50))\n",
        "        non_promoters.append((seq, 0))\n",
        "    \n",
        "    data = promoters + non_promoters\n",
        "    np.random.shuffle(data)\n",
        "    \n",
        "    sequences, labels = zip(*data)\n",
        "    X = torch.stack([encode_sequence(s) for s in sequences])\n",
        "    y = torch.tensor(labels, dtype=torch.float32)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Generate data\n",
        "import numpy as np\n",
        "X, y = generate_promoter_data(n_samples=500)\n",
        "print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
        "\n",
        "# Split data\n",
        "n_train = 400\n",
        "X_train, y_train = X[:n_train], y[:n_train]\n",
        "X_val, y_val = X[n_train:], y[n_train:]\n",
        "\n",
        "# Build classifier\n",
        "class PromoterClassifier(nn.Module):\n",
        "    def __init__(self, input_size=200, hidden_sizes=[128, 64], dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Build layers\n",
        "        layers = []\n",
        "        in_size = input_size\n",
        "        \n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_size = hidden_size\n",
        "        \n",
        "        layers.append(nn.Linear(in_size, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze()\n",
        "\n",
        "# Create model\n",
        "model = PromoterClassifier(input_size=200, hidden_sizes=[128, 64], dropout=0.3)\n",
        "print(f\"Model:\\\\n{model}\\\\n\")\n",
        "\n",
        "# Training with validation\n",
        "def train_with_validation(model, X_train, y_train, X_val, y_val, \n",
        "                          epochs=100, lr=0.001):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        y_pred = model(X_train)\n",
        "        train_loss = criterion(y_pred, y_train)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_acc = ((y_pred > 0.5) == y_train).float().mean()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_val_pred = model(X_val)\n",
        "            val_loss = criterion(y_val_pred, y_val)\n",
        "            val_acc = ((y_val_pred > 0.5) == y_val).float().mean()\n",
        "        \n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        train_accs.append(train_acc.item())\n",
        "        val_accs.append(val_acc.item())\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
        "                  f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "    \n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "# Train\n",
        "results = train_with_validation(model, X_train, y_train, X_val, y_val, epochs=100)\n",
        "train_losses, val_losses, train_accs, val_accs = results\n",
        "\n",
        "# Visualize\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "ax1.plot(train_losses, label='Train')\n",
        "ax1.plot(val_losses, label='Validation')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Loss Curves')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(train_accs, label='Train')\n",
        "ax2.plot(val_accs, label='Validation')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy Curves')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion matrix on validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = (model(X_val) > 0.5).long()\n",
        "    \n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "ax3.imshow(cm, cmap='Blues')\n",
        "ax3.set_title('Confusion Matrix (Validation)')\n",
        "ax3.set_xlabel('Predicted')\n",
        "ax3.set_ylabel('Actual')\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax3.text(j, i, cm[i,j], ha='center', va='center')\n",
        "\n",
        "# ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "with torch.no_grad():\n",
        "    y_scores = model(X_val).numpy()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_val, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "ax4.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')\n",
        "ax4.plot([0, 1], [0, 1], 'k--')\n",
        "ax4.set_xlabel('False Positive Rate')\n",
        "ax4.set_ylabel('True Positive Rate')\n",
        "ax4.set_title('ROC Curve')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\nFinal Validation Accuracy: {val_accs[-1]:.4f}\")\n",
        "print(f\"Final Validation AUC: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 3: Activation Functions Comparison\n",
        "\n",
        "### Task: Compare different activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPWithActivation(nn.Module):\n",
        "    def __init__(self, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(3, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 1)\n",
        "        self.activation_name = activation\n",
        "        \n",
        "        # Select activation\n",
        "        if activation == 'relu':\n",
        "            self.activation = F.relu\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = torch.sigmoid\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = torch.tanh\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = F.leaky_relu\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {activation}\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Generate nonlinear data\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand(300, 3)\n",
        "y = torch.sin(X[:,0] * 3) + torch.cos(X[:,1] * 2) + torch.randn(300) * 0.1\n",
        "\n",
        "# Compare activations\n",
        "activations = ['relu', 'sigmoid', 'tanh', 'leaky_relu']\n",
        "results = {}\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"\\\\nTraining with {act}...\")\n",
        "    model = MLPWithActivation(activation=act)\n",
        "    losses = train(model, X, y, epochs=100, lr=0.01)\n",
        "    results[act] = losses\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "for act, losses in results.items():\n",
        "    plt.plot(losses, label=act, alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Activation Function Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 4: Batch Normalization\n",
        "\n",
        "### Task: Understand batch normalization benefits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPWithBatchNorm(nn.Module):\n",
        "    def __init__(self, use_bn=True):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.bn1 = nn.BatchNorm1d(50) if use_bn else nn.Identity()\n",
        "        self.fc2 = nn.Linear(50, 30)\n",
        "        self.bn2 = nn.BatchNorm1d(30) if use_bn else nn.Identity()\n",
        "        self.fc3 = nn.Linear(30, 1)\n",
        "        self.use_bn = use_bn\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn1(F.relu(self.fc1(x)))\n",
        "        x = self.bn2(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(500, 10) * 5  # Large scale variation\n",
        "y = (X.sum(dim=1) + torch.randn(500) * 2)\n",
        "\n",
        "# Compare with and without BN\n",
        "models = {\n",
        "    'Without BatchNorm': MLPWithBatchNorm(use_bn=False),\n",
        "    'With BatchNorm': MLPWithBatchNorm(use_bn=True)\n",
        "}\n",
        "\n",
        "results_bn = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    losses = train(model, X, y, epochs=100, lr=0.01)\n",
        "    results_bn[name] = losses\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for name, losses in results_bn.items():\n",
        "    plt.plot(losses, label=name, linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Effect of Batch Normalization')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 5: Dropout Regularization\n",
        "\n",
        "### Task: Prevent overfitting with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DropoutMLP(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(5, 100)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(50, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Small dataset (prone to overfitting)\n",
        "torch.manual_seed(42)\n",
        "X_small = torch.randn(50, 5)\n",
        "y_small = (X_small.sum(dim=1) + torch.randn(50) * 0.5)\n",
        "\n",
        "X_val = torch.randn(100, 5)\n",
        "y_val = (X_val.sum(dim=1) + torch.randn(100) * 0.5)\n",
        "\n",
        "# Compare dropout rates\n",
        "dropout_rates = [0.0, 0.2, 0.5]\n",
        "dropout_results = {}\n",
        "\n",
        "for rate in dropout_rates:\n",
        "    print(f\"\\\\nDropout rate: {rate}\")\n",
        "    model = DropoutMLP(dropout_rate=rate)\n",
        "    train_losses, val_losses, _, _ = train_with_validation(\n",
        "        model, X_small, y_small, X_val, y_val, epochs=200, lr=0.01\n",
        "    )\n",
        "    dropout_results[rate] = (train_losses, val_losses)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for rate, (train_losses, val_losses) in dropout_results.items():\n",
        "    ax1.plot(train_losses, label=f'Dropout {rate}', alpha=0.7)\n",
        "    ax2.plot(val_losses, label=f'Dropout {rate}', alpha=0.7)\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Validation Loss (Overfitting Check)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83e\uddea Exercise 6: Weight Initialization\n",
        "\n",
        "### Task: Understand importance of proper initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InitializedMLP(nn.Module):\n",
        "    def __init__(self, init_method='default'):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.fc2 = nn.Linear(50, 30)\n",
        "        self.fc3 = nn.Linear(30, 1)\n",
        "        \n",
        "        # Initialize weights\n",
        "        if init_method == 'zeros':\n",
        "            self._init_zeros()\n",
        "        elif init_method == 'xavier':\n",
        "            self._init_xavier()\n",
        "        elif init_method == 'kaiming':\n",
        "            self._init_kaiming()\n",
        "        # else: use PyTorch default\n",
        "    \n",
        "    def _init_zeros(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.zeros_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def _init_xavier(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def _init_kaiming(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(200, 10)\n",
        "y = X.sum(dim=1) + torch.randn(200)\n",
        "\n",
        "# Compare initialization methods\n",
        "init_methods = ['default', 'xavier', 'kaiming']  # Skip 'zeros' - it won't learn!\n",
        "init_results = {}\n",
        "\n",
        "for method in init_methods:\n",
        "    print(f\"\\\\nInitialization: {method}\")\n",
        "    model = InitializedMLP(init_method=method)\n",
        "    losses = train(model, X, y, epochs=100, lr=0.01)\n",
        "    init_results[method] = losses\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for method, losses in init_results.items():\n",
        "    plt.plot(losses, label=method, linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Effect of Weight Initialization')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83c\udfaf Challenge 1: Multi-Task Learning\n",
        "\n",
        "### Task: Predict both expression level AND tissue type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "    \\\"\\\"\\\"\n",
        "    Multi-task model for genomics:\n",
        "    - Task 1: Predict gene expression (regression)\n",
        "    - Task 2: Predict tissue type (classification)\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, input_size, hidden_size, n_tissues):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Shared layers\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Task-specific heads\n",
        "        self.expression_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 1)\n",
        "        )\n",
        "        \n",
        "        self.tissue_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, n_tissues)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        shared_features = self.shared(x)\n",
        "        expression = self.expression_head(shared_features)\n",
        "        tissue_logits = self.tissue_head(shared_features)\n",
        "        return expression.squeeze(), tissue_logits\n",
        "\n",
        "# Generate multi-task data\n",
        "torch.manual_seed(42)\n",
        "n_samples = 500\n",
        "n_features = 20\n",
        "n_tissues = 3\n",
        "\n",
        "X = torch.randn(n_samples, n_features)\n",
        "tissue_types = torch.randint(0, n_tissues, (n_samples,))\n",
        "\n",
        "# Expression depends on features and tissue\n",
        "expression = X.sum(dim=1)\n",
        "for i in range(n_tissues):\n",
        "    mask = tissue_types == i\n",
        "    expression[mask] += (i - 1) * 2  # Tissue effect\n",
        "\n",
        "expression += torch.randn(n_samples) * 0.5\n",
        "\n",
        "print(f\"Data: {X.shape}\")\n",
        "print(f\"Expression: {expression.shape}\")\n",
        "print(f\"Tissues: {tissue_types.shape}\")\n",
        "print(f\"Tissue distribution: {torch.bincount(tissue_types)}\")\n",
        "\n",
        "# Create model\n",
        "model = MultiTaskModel(input_size=n_features, hidden_size=64, n_tissues=n_tissues)\n",
        "\n",
        "# Multi-task training\n",
        "def train_multitask(model, X, y_expr, y_tissue, epochs=100, lr=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    expr_criterion = nn.MSELoss()\n",
        "    tissue_criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    history = {'expr_loss': [], 'tissue_loss': [], 'total_loss': [], 'accuracy': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        \n",
        "        # Forward\n",
        "        expr_pred, tissue_logits = model(X)\n",
        "        \n",
        "        # Compute losses\n",
        "        expr_loss = expr_criterion(expr_pred, y_expr)\n",
        "        tissue_loss = tissue_criterion(tissue_logits, y_tissue)\n",
        "        total_loss = expr_loss + tissue_loss  # Can add weights here\n",
        "        \n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Metrics\n",
        "        tissue_pred = tissue_logits.argmax(dim=1)\n",
        "        accuracy = (tissue_pred == y_tissue).float().mean()\n",
        "        \n",
        "        history['expr_loss'].append(expr_loss.item())\n",
        "        history['tissue_loss'].append(tissue_loss.item())\n",
        "        history['total_loss'].append(total_loss.item())\n",
        "        history['accuracy'].append(accuracy.item())\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                  f\"Expr Loss={expr_loss:.4f}, \"\n",
        "                  f\"Tissue Loss={tissue_loss:.4f}, \"\n",
        "                  f\"Accuracy={accuracy:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Train\n",
        "history = train_multitask(model, X, expression, tissue_types, epochs=100)\n",
        "\n",
        "# Visualize\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "ax1.plot(history['expr_loss'])\n",
        "ax1.set_title('Expression Loss (Regression)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('MSE Loss')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history['tissue_loss'])\n",
        "ax2.set_title('Tissue Loss (Classification)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('CrossEntropy Loss')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "ax3.plot(history['total_loss'])\n",
        "ax3.set_title('Total Loss')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Combined Loss')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "ax4.plot(history['accuracy'])\n",
        "ax4.set_title('Tissue Classification Accuracy')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Accuracy')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    expr_pred, tissue_logits = model(X)\n",
        "    expr_mse = F.mse_loss(expr_pred, expression)\n",
        "    tissue_acc = (tissue_logits.argmax(dim=1) == tissue_types).float().mean()\n",
        "    \n",
        "print(f\"\\\\nFinal Performance:\")\n",
        "print(f\"Expression MSE: {expr_mse:.4f}\")\n",
        "print(f\"Tissue Accuracy: {tissue_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \u2705 Self-Assessment\n",
        "\n",
        "Before moving to Session 4, ensure you can:\n",
        "\n",
        "- [ ] Explain why we subclass `nn.Module`\n",
        "- [ ] Define `__init__` and `forward` methods correctly\n",
        "- [ ] Build multi-layer networks with various architectures\n",
        "- [ ] Choose appropriate activation functions\n",
        "- [ ] Apply dropout for regularization\n",
        "- [ ] Use batch normalization effectively\n",
        "- [ ] Initialize weights properly\n",
        "- [ ] Build multi-task models\n",
        "- [ ] Use `model.train()` and `model.eval()` correctly\n",
        "- [ ] Access and inspect model parameters\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcdd Key Takeaways\n",
        "\n",
        "### The nn.Module Pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "1. Subclass nn.Module\n",
        "2. Call super().__init__() in constructor\n",
        "3. Define layers in __init__\n",
        "4. Define computation in forward\n",
        "5. Never call forward directly\n",
        "6. Use model.train() and model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation Functions:\n",
        "- **ReLU**: Default choice, fast, effective\n",
        "- **Sigmoid**: Output (0,1), binary classification\n",
        "- **Tanh**: Output (-1,1), sometimes better than sigmoid\n",
        "- **LeakyReLU**: Prevents dying ReLU problem\n",
        "\n",
        "### Regularization:\n",
        "- **Dropout**: Randomly drop neurons during training\n",
        "- **BatchNorm**: Normalize activations, faster training\n",
        "- **Weight decay**: Add L2 penalty in optimizer\n",
        "\n",
        "### Initialization:\n",
        "- **Xavier/Glorot**: Good for sigmoid/tanh\n",
        "- **Kaiming/He**: Good for ReLU\n",
        "- **Default PyTorch**: Usually fine\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\ude80 Next Steps\n",
        "\n",
        "**Session 4** will cover:\n",
        "- Convolutional layers for sequences\n",
        "- 1D convolutions for DNA\n",
        "- Motif detection\n",
        "- CNN architectures for genomics\n",
        "\n",
        "**Prepare by**:\n",
        "- Understanding how convolutions work\n",
        "- Reviewing Session 1 (sequence encoding)\n",
        "- Reading Deep Learning with PyTorch Ch. 8\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcda Additional Resources\n",
        "\n",
        "- PyTorch nn.Module docs: https://pytorch.org/docs/stable/nn.html\n",
        "- Activation functions: https://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
        "- Weight initialization: https://pytorch.org/docs/stable/nn.init.html\n",
        "- Understanding dropout: https://jmlr.org/papers/v15/srivastava14a.html\n",
        "\n",
        "---\n",
        "\n",
        "*Session 3 Complete! You now know how to build deep neural networks in PyTorch!*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}