{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 5: Recurrent Neural Networks for Sequences\n",
        "**Book Ref**: Deep Learning PyTorch Ch. 9, Gen AI Ch. 8 | **Duration**: 3-4 hours\n",
        "\n",
        "## Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
        "nn.GRU(input_size, hidden_size, num_layers)\n",
        "nn.utils.rnn.pack_padded_sequence()\n",
        "nn.utils.rnn.pad_packed_sequence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Basic LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProteinLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size=20, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 64)\n",
        "        self.lstm = nn.LSTM(64, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 3)  # 3 secondary structures\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, (h, c) = self.lstm(x)\n",
        "        return self.fc(lstm_out)\n",
        "\n",
        "model = ProteinLSTM()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(64, 128, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(256, 1)  # 128*2 for bidirectional\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return torch.sigmoid(self.fc(lstm_out[:, -1, :]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Variable Length Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "def handle_variable_length(sequences, lengths):\n",
        "    packed = pack_padded_sequence(sequences, lengths, \n",
        "                                   batch_first=True, enforce_sorted=False)\n",
        "    lstm_out, _ = model.lstm(packed)\n",
        "    unpacked, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "    return unpacked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Complete exercises in notebook format*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}