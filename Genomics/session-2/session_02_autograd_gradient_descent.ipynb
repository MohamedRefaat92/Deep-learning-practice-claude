{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 2: Autograd and Gradient Descent\n## PyTorch for Genomics - Learning from Data\n\n**Source**: Deep Learning with PyTorch (Second Edition), Chapter 5 - \"The mechanics of learning\"\n\n**Duration**: 2-3 hours  \n**Difficulty**: Beginner to Intermediate  \n**Prerequisites**: Completion of Session 1, understanding of basic calculus concepts\n\n---\n\n## \ud83c\udfaf Learning Objectives\n\nBy the end of this session, you will:\n1. Understand how PyTorch's automatic differentiation (autograd) works\n2. Implement gradient descent from scratch and with PyTorch optimizers\n3. Build complete training loops for genomics models\n4. Properly split data into training and validation sets\n5. Recognize and prevent overfitting\n6. Compare different optimization strategies (SGD vs Adam)\n7. Visualize learning curves and diagnose training issues\n\n---\n\n## \ud83d\udcda Theory Review\n\n### What is Learning?\n\n**Learning** in machine learning means adjusting model parameters to minimize a loss function. Think of it as:\n1. **Model**: A function that makes predictions (e.g., predicting if a variant is pathogenic)\n2. **Loss**: A measure of how wrong the predictions are\n3. **Gradient**: The direction to adjust parameters to reduce loss\n4. **Optimizer**: The strategy for updating parameters using gradients\n\n### The Learning Process\n\n```\n1. Forward Pass:  inputs \u2192 model \u2192 predictions\n2. Loss Calculation: compare predictions with truth\n3. Backward Pass: compute gradients (how to improve)\n4. Update: adjust parameters to reduce loss\n5. Repeat until loss is minimized\n```\n\n### Key Concepts\n\n**Gradient Descent**: Move parameters in the direction that reduces loss\n- Learning Rate: How big each step should be\n- Too large: Training unstable, loss oscillates\n- Too small: Training too slow, may get stuck\n\n**Autograd**: PyTorch automatically computes derivatives\n- Tracks operations in a computation graph\n- Computes gradients via backpropagation\n- Essential for training neural networks\n\n---\n\n## \ud83e\uddea Exercise 1: Manual Gradient Descent for Gene Expression\n\n### Part A: Problem Setup\n\nWe'll predict gene expression levels from a simple linear model based on transcription factor binding scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate genomics data\n# True relationship: expression = 2.5 * TF_binding + 1.0 + noise\ntorch.manual_seed(42)\n\n# Transcription factor binding scores (normalized 0-1)\ntf_binding = torch.linspace(0, 1, 50)\n\n# True gene expression levels (with noise)\ntrue_weight = 2.5\ntrue_bias = 1.0\ngene_expression = true_weight * tf_binding + true_bias + torch.randn(50) * 0.2\n\nprint(f\"TF binding scores shape: {tf_binding.shape}\")\nprint(f\"Gene expression shape: {gene_expression.shape}\")\nprint(f\"\\nFirst 5 samples:\")\nprint(f\"TF binding: {tf_binding[:5]}\")\nprint(f\"Expression: {gene_expression[:5]}\")\n\n# Visualize the data\nplt.figure(figsize=(10, 6))\nplt.scatter(tf_binding, gene_expression, alpha=0.6, label='Data')\nplt.xlabel('TF Binding Score')\nplt.ylabel('Gene Expression Level')\nplt.title('Transcription Factor Binding vs Gene Expression')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Define Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, weight, bias):\n    \"\"\"\n    Simple linear model: y = w * x + b\n    \n    Args:\n        x: Input tensor (TF binding scores)\n        weight: Model parameter (slope)\n        bias: Model parameter (intercept)\n    \n    Returns:\n        Predictions\n    \"\"\"\n    return weight * x + bias\n\ndef mse_loss(predictions, targets):\n    \"\"\"\n    Mean Squared Error loss function.\n    \n    Args:\n        predictions: Model outputs\n        targets: True values\n        \n    Returns:\n        Average squared difference\n    \"\"\"\n    squared_diff = (predictions - targets) ** 2\n    return squared_diff.mean()\n\n# Initialize parameters randomly\nweight = torch.randn(1)\nbias = torch.randn(1)\n\nprint(f\"Initial weight: {weight.item():.4f}\")\nprint(f\"Initial bias: {bias.item():.4f}\")\n\n# Make initial predictions\ninitial_predictions = linear_model(tf_binding, weight, bias)\ninitial_loss = mse_loss(initial_predictions, gene_expression)\n\nprint(f\"\\nInitial loss: {initial_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Manual Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y, weight, bias):\n    \"\"\"\n    Manually compute gradients of MSE loss.\n    \n    For loss L = mean((y_pred - y_true)^2)\n    Where y_pred = w*x + b\n    \n    Gradients:\n    dL/dw = mean(2 * (y_pred - y_true) * x)\n    dL/db = mean(2 * (y_pred - y_true))\n    \n    Args:\n        x: Input features\n        y: True targets\n        weight: Current weight parameter\n        bias: Current bias parameter\n        \n    Returns:\n        Tuple of (weight_gradient, bias_gradient)\n    \"\"\"\n    # Task 1.1: Implement manual gradient computation\n    # YOUR CODE HERE\n    \n    # Forward pass\n    y_pred = linear_model(x, weight, bias)\n    \n    # Compute error\n    error = y_pred - y\n    \n    # Compute gradients\n    n = x.shape[0]\n    weight_grad = (2 / n) * (error * x).sum()\n    bias_grad = (2 / n) * error.sum()\n    \n    return weight_grad, bias_grad\n\n# Test gradient computation\nw_grad, b_grad = compute_gradients(tf_binding, gene_expression, weight, bias)\nprint(f\"Weight gradient: {w_grad.item():.4f}\")\nprint(f\"Bias gradient: {b_grad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Training Loop with Manual Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_gradient_descent(x, y, epochs=100, learning_rate=0.1):\n    \"\"\"\n    Train linear model using manually computed gradients.\n    \n    Args:\n        x: Input features\n        y: Targets\n        epochs: Number of training iterations\n        learning_rate: Step size for parameter updates\n        \n    Returns:\n        Tuple of (final_weight, final_bias, loss_history)\n    \"\"\"\n    # Task 1.2: Implement training loop\n    # YOUR CODE HERE\n    \n    # Initialize parameters\n    weight = torch.randn(1)\n    bias = torch.randn(1)\n    \n    loss_history = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = linear_model(x, weight, bias)\n        loss = mse_loss(predictions, y)\n        loss_history.append(loss.item())\n        \n        # Compute gradients manually\n        w_grad, b_grad = compute_gradients(x, y, weight, bias)\n        \n        # Update parameters (gradient descent step)\n        weight = weight - learning_rate * w_grad\n        bias = bias - learning_rate * b_grad\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}, \"\n                  f\"Weight = {weight.item():.4f}, Bias = {bias.item():.4f}\")\n    \n    return weight, bias, loss_history\n\n# Train the model\nprint(\"Training with manual gradient descent:\\n\")\nfinal_weight, final_bias, loss_history = train_manual_gradient_descent(\n    tf_binding, gene_expression, epochs=100, learning_rate=0.1\n)\n\nprint(f\"\\nFinal parameters:\")\nprint(f\"Weight: {final_weight.item():.4f} (true: {true_weight})\")\nprint(f\"Bias: {final_bias.item():.4f} (true: {true_bias})\")\n\n# Visualize training progress\nplt.figure(figsize=(10, 6))\nplt.plot(loss_history)\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Training Loss Over Time')\nplt.grid(True, alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83e\uddea Exercise 2: PyTorch Autograd\n\n### Part A: Understanding requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n\n# Task 2.1: Experiment with requires_grad\n# YOUR CODE HERE\n\n# Create tensors with and without gradient tracking\nx_no_grad = torch.tensor([1.0, 2.0, 3.0])\nx_with_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\nprint(\"Tensor without gradient tracking:\")\nprint(f\"  Value: {x_no_grad}\")\nprint(f\"  requires_grad: {x_no_grad.requires_grad}\")\nprint(f\"  grad: {x_no_grad.grad}\")\n\nprint(\"\\nTensor with gradient tracking:\")\nprint(f\"  Value: {x_with_grad}\")\nprint(f\"  requires_grad: {x_with_grad.requires_grad}\")\nprint(f\"  grad: {x_with_grad.grad}\")\n\n# Perform operation\ny = (x_with_grad ** 2).sum()\nprint(f\"\\nAfter operation y = sum(x^2):\")\nprint(f\"  y value: {y.item()}\")\nprint(f\"  y requires_grad: {y.requires_grad}\")\n\n# Compute gradients\ny.backward()\n\nprint(f\"\\nAfter calling y.backward():\")\nprint(f\"  x gradient (dy/dx): {x_with_grad.grad}\")\nprint(f\"  Expected: 2*x = {2 * x_with_grad.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Autograd for Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_autograd(x, y, epochs=100, learning_rate=0.1):\n    \"\"\"\n    Train linear model using PyTorch autograd.\n    \n    Args:\n        x: Input features\n        y: Targets\n        epochs: Number of training iterations\n        learning_rate: Step size for parameter updates\n        \n    Returns:\n        Tuple of (final_weight, final_bias, loss_history)\n    \"\"\"\n    # Task 2.2: Implement training with autograd\n    # YOUR CODE HERE\n    \n    # Initialize parameters with gradient tracking\n    weight = torch.randn(1, requires_grad=True)\n    bias = torch.randn(1, requires_grad=True)\n    \n    loss_history = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = linear_model(x, weight, bias)\n        loss = mse_loss(predictions, y)\n        loss_history.append(loss.item())\n        \n        # CRITICAL: Zero gradients from previous iteration\n        if weight.grad is not None:\n            weight.grad.zero_()\n        if bias.grad is not None:\n            bias.grad.zero_()\n        \n        # Backward pass - autograd computes gradients!\n        loss.backward()\n        \n        # Update parameters (no gradient tracking for this operation)\n        with torch.no_grad():\n            weight -= learning_rate * weight.grad\n            bias -= learning_rate * bias.grad\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}, \"\n                  f\"Weight = {weight.item():.4f}, Bias = {bias.item():.4f}\")\n    \n    return weight, bias, loss_history\n\n# Train the model\nprint(\"Training with PyTorch autograd:\\n\")\nfinal_weight_auto, final_bias_auto, loss_history_auto = train_with_autograd(\n    tf_binding, gene_expression, epochs=100, learning_rate=0.1\n)\n\nprint(f\"\\nFinal parameters:\")\nprint(f\"Weight: {final_weight_auto.item():.4f} (true: {true_weight})\")\nprint(f\"Bias: {final_bias_auto.item():.4f} (true: {true_bias})\")\n\n# Compare with manual gradients\nplt.figure(figsize=(10, 6))\nplt.plot(loss_history, label='Manual Gradients', alpha=0.7)\nplt.plot(loss_history_auto, label='Autograd', alpha=0.7, linestyle='--')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Training Loss: Manual vs Autograd')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Understanding the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Visualize computation graph\n# YOUR CODE HERE\n\n# Create simple computation\nx = torch.tensor([2.0], requires_grad=True)\na = x * 3\nb = a + 5\nc = b ** 2\nloss = c.mean()\n\nprint(\"Computation graph:\")\nprint(f\"x = {x.item()}\")\nprint(f\"a = x * 3 = {a.item()}\")\nprint(f\"b = a + 5 = {b.item()}\")\nprint(f\"c = b^2 = {c.item()}\")\nprint(f\"loss = mean(c) = {loss.item()}\")\n\n# Compute gradient\nloss.backward()\n\nprint(f\"\\nGradient dl/dx = {x.grad.item()}\")\n\n# Manual verification using chain rule\n# dl/dx = dl/dc * dc/db * db/da * da/dx\n# dl/dc = 1 (derivative of mean)\n# dc/db = 2*b\n# db/da = 1\n# da/dx = 3\nmanual_grad = 1.0 * 2 * b.item() * 1.0 * 3.0\nprint(f\"Manual calculation: {manual_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83e\uddea Exercise 3: PyTorch Optimizers\n\n### Part A: SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n\ndef train_with_optimizer(x, y, optimizer_class, epochs=100, learning_rate=0.1, **optimizer_kwargs):\n    \"\"\"\n    Train linear model using PyTorch optimizer.\n    \n    Args:\n        x: Input features\n        y: Targets\n        optimizer_class: Optimizer class (e.g., optim.SGD, optim.Adam)\n        epochs: Number of training iterations\n        learning_rate: Learning rate\n        **optimizer_kwargs: Additional optimizer arguments\n        \n    Returns:\n        Tuple of (parameters, loss_history)\n    \"\"\"\n    # Task 3.1: Implement training with PyTorch optimizer\n    # YOUR CODE HERE\n    \n    # Initialize parameters\n    weight = torch.randn(1, requires_grad=True)\n    bias = torch.randn(1, requires_grad=True)\n    \n    # Create optimizer\n    optimizer = optimizer_class([weight, bias], lr=learning_rate, **optimizer_kwargs)\n    \n    loss_history = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = linear_model(x, weight, bias)\n        loss = mse_loss(predictions, y)\n        loss_history.append(loss.item())\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}, \"\n                  f\"Weight = {weight.item():.4f}, Bias = {bias.item():.4f}\")\n    \n    return (weight, bias), loss_history\n\n# Train with SGD\nprint(\"Training with SGD optimizer:\\n\")\nparams_sgd, loss_sgd = train_with_optimizer(\n    tf_binding, gene_expression, \n    optim.SGD, \n    epochs=100, \n    learning_rate=0.1\n)\n\nweight_sgd, bias_sgd = params_sgd\nprint(f\"\\nFinal SGD parameters:\")\nprint(f\"Weight: {weight_sgd.item():.4f} (true: {true_weight})\")\nprint(f\"Bias: {bias_sgd.item():.4f} (true: {true_bias})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2: Train with Adam optimizer\n# YOUR CODE HERE\n\nprint(\"\\nTraining with Adam optimizer:\\n\")\nparams_adam, loss_adam = train_with_optimizer(\n    tf_binding, gene_expression, \n    optim.Adam, \n    epochs=100, \n    learning_rate=0.1\n)\n\nweight_adam, bias_adam = params_adam\nprint(f\"\\nFinal Adam parameters:\")\nprint(f\"Weight: {weight_adam.item():.4f} (true: {true_weight})\")\nprint(f\"Bias: {bias_adam.item():.4f} (true: {true_bias})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Comparing Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3: Compare different optimizers\n# YOUR CODE HERE\n\n# Test multiple learning rates with SGD\nlearning_rates = [0.01, 0.1, 0.5]\nplt.figure(figsize=(15, 5))\n\nfor i, lr in enumerate(learning_rates, 1):\n    _, loss_history = train_with_optimizer(\n        tf_binding, gene_expression, \n        optim.SGD, \n        epochs=100, \n        learning_rate=lr,\n        verbose=False\n    )\n    \n    plt.subplot(1, 3, i)\n    plt.plot(loss_history)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(f'SGD with lr={lr}')\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Compare optimizers at same learning rate\nplt.figure(figsize=(10, 6))\n\noptimizers = [\n    (optim.SGD, 'SGD'),\n    (optim.Adam, 'Adam'),\n    (optim.RMSprop, 'RMSprop')\n]\n\nfor opt_class, name in optimizers:\n    _, loss_history = train_with_optimizer(\n        tf_binding, gene_expression, \n        opt_class, \n        epochs=100, \n        learning_rate=0.1,\n        verbose=False\n    )\n    plt.plot(loss_history, label=name, alpha=0.7)\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Optimizer Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83e\uddea Exercise 4: Variant Classification (Binary)\n\nBuild a classifier to predict if genetic variants are pathogenic or benign.\n\n### Part A: Generate Synthetic Variant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn.functional as F\n\ndef generate_variant_data(n_samples=1000, seed=42):\n    \"\"\"\n    Generate synthetic variant data for binary classification.\n    \n    Features:\n        - Conservation score (0-1)\n        - Allele frequency (0-1)\n        - CADD score (0-40)\n        - PolyPhen score (0-1)\n        - SIFT score (0-1)\n    \n    Labels:\n        - 0: Benign\n        - 1: Pathogenic\n    \"\"\"\n    torch.manual_seed(seed)\n    \n    # Pathogenic variants (label=1)\n    n_pathogenic = n_samples // 2\n    pathogenic_features = torch.stack([\n        torch.randn(n_pathogenic) * 0.1 + 0.9,  # High conservation\n        torch.randn(n_pathogenic) * 0.05 + 0.02,  # Low frequency\n        torch.randn(n_pathogenic) * 5 + 25,  # High CADD\n        torch.randn(n_pathogenic) * 0.1 + 0.8,  # High PolyPhen\n        torch.randn(n_pathogenic) * 0.1 + 0.1,  # Low SIFT (damaging)\n    ]).T\n    pathogenic_labels = torch.ones(n_pathogenic)\n    \n    # Benign variants (label=0)\n    n_benign = n_samples - n_pathogenic\n    benign_features = torch.stack([\n        torch.randn(n_benign) * 0.1 + 0.3,  # Low conservation\n        torch.randn(n_benign) * 0.1 + 0.5,  # High frequency\n        torch.randn(n_benign) * 3 + 10,  # Low CADD\n        torch.randn(n_benign) * 0.1 + 0.2,  # Low PolyPhen\n        torch.randn(n_benign) * 0.1 + 0.8,  # High SIFT (tolerated)\n    ]).T\n    benign_labels = torch.zeros(n_benign)\n    \n    # Combine and shuffle\n    features = torch.cat([pathogenic_features, benign_features])\n    labels = torch.cat([pathogenic_labels, benign_labels])\n    \n    # Normalize features to [0, 1] range\n    features = (features - features.min(dim=0)[0]) / (features.max(dim=0)[0] - features.min(dim=0)[0])\n    \n    # Shuffle\n    indices = torch.randperm(n_samples)\n    features = features[indices]\n    labels = labels[indices]\n    \n    return features, labels\n\n# Generate data\nfeatures, labels = generate_variant_data(n_samples=1000)\n\nprint(f\"Features shape: {features.shape}\")\nprint(f\"Labels shape: {labels.shape}\")\nprint(f\"\\nFeature names: Conservation, Frequency, CADD, PolyPhen, SIFT\")\nprint(f\"First 3 samples:\")\nprint(features[:3])\nprint(f\"Labels: {labels[:3]}\")\nprint(f\"\\nClass distribution:\")\nprint(f\"Benign (0): {(labels == 0).sum().item()}\")\nprint(f\"Pathogenic (1): {(labels == 1).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model(x, weights, bias):\n    \"\"\"\n    Logistic regression: sigmoid(w * x + b)\n    \n    Args:\n        x: Input features (n_samples, n_features)\n        weights: Model weights (n_features,)\n        bias: Model bias (scalar)\n        \n    Returns:\n        Probabilities (n_samples,)\n    \"\"\"\n    # Task 4.1: Implement logistic regression\n    # YOUR CODE HERE\n    logits = x @ weights + bias\n    probabilities = torch.sigmoid(logits)\n    return probabilities\n\ndef binary_cross_entropy_loss(predictions, targets):\n    \"\"\"\n    Binary cross-entropy loss.\n    \n    Loss = -mean(y*log(p) + (1-y)*log(1-p))\n    \n    Args:\n        predictions: Predicted probabilities (0-1)\n        targets: True labels (0 or 1)\n        \n    Returns:\n        Loss value\n    \"\"\"\n    # Task 4.2: Implement BCE loss\n    # YOUR CODE HERE\n    epsilon = 1e-7  # For numerical stability\n    predictions = torch.clamp(predictions, epsilon, 1 - epsilon)\n    loss = -(targets * torch.log(predictions) + (1 - targets) * torch.log(1 - predictions))\n    return loss.mean()\n\n# Initialize parameters\nn_features = features.shape[1]\nweights = torch.randn(n_features, requires_grad=True) * 0.1\nbias = torch.zeros(1, requires_grad=True)\n\nprint(f\"Model initialized:\")\nprint(f\"  Weights shape: {weights.shape}\")\nprint(f\"  Bias shape: {bias.shape}\")\n\n# Test forward pass\ntest_probs = logistic_model(features[:5], weights, bias)\nprint(f\"\\nTest predictions: {test_probs}\")\ntest_loss = binary_cross_entropy_loss(test_probs, labels[:5])\nprint(f\"Test loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(features, labels, epochs=100, learning_rate=0.1):\n    \"\"\"\n    Train logistic regression classifier.\n    \n    Args:\n        features: Input features\n        labels: True labels\n        epochs: Number of training iterations\n        learning_rate: Learning rate\n        \n    Returns:\n        Tuple of (weights, bias, loss_history, accuracy_history)\n    \"\"\"\n    # Task 4.3: Implement classifier training\n    # YOUR CODE HERE\n    \n    n_features = features.shape[1]\n    weights = torch.randn(n_features, requires_grad=True) * 0.1\n    bias = torch.zeros(1, requires_grad=True)\n    \n    optimizer = optim.Adam([weights, bias], lr=learning_rate)\n    \n    loss_history = []\n    accuracy_history = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = logistic_model(features, weights, bias)\n        loss = binary_cross_entropy_loss(predictions, labels)\n        \n        # Calculate accuracy\n        predicted_classes = (predictions > 0.5).float()\n        accuracy = (predicted_classes == labels).float().mean()\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Record metrics\n        loss_history.append(loss.item())\n        accuracy_history.append(accuracy.item())\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}, \"\n                  f\"Accuracy = {accuracy.item():.4f}\")\n    \n    return weights, bias, loss_history, accuracy_history\n\n# Train classifier\nprint(\"Training variant classifier:\\n\")\nweights, bias, loss_hist, acc_hist = train_classifier(\n    features, labels, epochs=100, learning_rate=0.1\n)\n\n# Visualize training\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(loss_hist)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss (BCE)')\nax1.set_title('Training Loss')\nax1.grid(True, alpha=0.3)\n\nax2.plot(acc_hist)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.set_title('Training Accuracy')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal accuracy: {acc_hist[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83e\uddea Exercise 5: Training and Validation Split\n\n### Part A: Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, train_ratio=0.8, seed=42):\n    \"\"\"\n    Split data into training and validation sets.\n    \n    Args:\n        features: Input features\n        labels: Labels\n        train_ratio: Fraction of data for training\n        seed: Random seed for reproducibility\n        \n    Returns:\n        Tuple of (train_features, train_labels, val_features, val_labels)\n    \"\"\"\n    # Task 5.1: Implement data splitting\n    # YOUR CODE HERE\n    \n    torch.manual_seed(seed)\n    \n    n_samples = features.shape[0]\n    n_train = int(n_samples * train_ratio)\n    \n    # Random permutation\n    indices = torch.randperm(n_samples)\n    train_indices = indices[:n_train]\n    val_indices = indices[n_train:]\n    \n    # Split data\n    train_features = features[train_indices]\n    train_labels = labels[train_indices]\n    val_features = features[val_indices]\n    val_labels = labels[val_indices]\n    \n    return train_features, train_labels, val_features, val_labels\n\n# Split the variant data\ntrain_feats, train_labs, val_feats, val_labs = split_data(features, labels)\n\nprint(f\"Training set:\")\nprint(f\"  Features: {train_feats.shape}\")\nprint(f\"  Labels: {train_labs.shape}\")\nprint(f\"  Pathogenic: {(train_labs == 1).sum().item()}\")\n\nprint(f\"\\nValidation set:\")\nprint(f\"  Features: {val_feats.shape}\")\nprint(f\"  Labels: {val_labs.shape}\")\nprint(f\"  Pathogenic: {(val_labs == 1).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Training with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(train_features, train_labels, val_features, val_labels, \n                          epochs=100, learning_rate=0.1):\n    \"\"\"\n    Train classifier with separate validation monitoring.\n    \n    Args:\n        train_features: Training features\n        train_labels: Training labels\n        val_features: Validation features\n        val_labels: Validation labels\n        epochs: Number of training iterations\n        learning_rate: Learning rate\n        \n    Returns:\n        Tuple of (weights, bias, train_history, val_history)\n    \"\"\"\n    # Task 5.2: Implement training with validation\n    # YOUR CODE HERE\n    \n    n_features = train_features.shape[1]\n    weights = torch.randn(n_features, requires_grad=True) * 0.1\n    bias = torch.zeros(1, requires_grad=True)\n    \n    optimizer = optim.Adam([weights, bias], lr=learning_rate)\n    \n    train_loss_history = []\n    train_acc_history = []\n    val_loss_history = []\n    val_acc_history = []\n    \n    for epoch in range(epochs):\n        # Training\n        train_preds = logistic_model(train_features, weights, bias)\n        train_loss = binary_cross_entropy_loss(train_preds, train_labels)\n        train_acc = ((train_preds > 0.5).float() == train_labels).float().mean()\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        # Validation (no gradient computation)\n        with torch.no_grad():\n            val_preds = logistic_model(val_features, weights, bias)\n            val_loss = binary_cross_entropy_loss(val_preds, val_labels)\n            val_acc = ((val_preds > 0.5).float() == val_labels).float().mean()\n        \n        # Record metrics\n        train_loss_history.append(train_loss.item())\n        train_acc_history.append(train_acc.item())\n        val_loss_history.append(val_loss.item())\n        val_acc_history.append(val_acc.item())\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}: \"\n                  f\"Train Loss = {train_loss.item():.4f}, Train Acc = {train_acc.item():.4f} | \"\n                  f\"Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc.item():.4f}\")\n    \n    return weights, bias, (train_loss_history, train_acc_history), (val_loss_history, val_acc_history)\n\n# Train with validation\nprint(\"Training with validation monitoring:\\n\")\nweights, bias, train_hist, val_hist = train_with_validation(\n    train_feats, train_labs, val_feats, val_labs, \n    epochs=100, learning_rate=0.1\n)\n\ntrain_loss, train_acc = train_hist\nval_loss, val_acc = val_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Visualizing Training vs Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.3: Create comprehensive visualization\n# YOUR CODE HERE\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Loss comparison\nax1.plot(train_loss, label='Training', alpha=0.7)\nax1.plot(val_loss, label='Validation', alpha=0.7)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss (BCE)')\nax1.set_title('Training vs Validation Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Accuracy comparison\nax2.plot(train_acc, label='Training', alpha=0.7)\nax2.plot(val_acc, label='Validation', alpha=0.7)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.set_title('Training vs Validation Accuracy')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Loss difference (overfitting indicator)\nloss_diff = [t - v for t, v in zip(train_loss, val_loss)]\nax3.plot(loss_diff)\nax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Train Loss - Val Loss')\nax3.set_title('Overfitting Indicator (should stay near 0)')\nax3.grid(True, alpha=0.3)\n\n# Final comparison\nmetrics = ['Train Loss', 'Val Loss', 'Train Acc', 'Val Acc']\nvalues = [train_loss[-1], val_loss[-1], train_acc[-1], val_acc[-1]]\ncolors = ['blue', 'orange', 'blue', 'orange']\nax4.bar(metrics, values, color=colors, alpha=0.7)\nax4.set_ylabel('Value')\nax4.set_title('Final Metrics Comparison')\nax4.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal Results:\")\nprint(f\"Training   - Loss: {train_loss[-1]:.4f}, Accuracy: {train_acc[-1]:.4f}\")\nprint(f\"Validation - Loss: {val_loss[-1]:.4f}, Accuracy: {val_acc[-1]:.4f}\")\nprint(f\"Gap        - Loss: {abs(train_loss[-1] - val_loss[-1]):.4f}, \"\n      f\"Accuracy: {abs(train_acc[-1] - val_acc[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83e\uddea Exercise 6: Detecting and Preventing Overfitting\n\n### Part A: Create Overfitting Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_capacity_control(train_feats, train_labs, val_feats, val_labs, \n                                 n_params_multiplier=1, epochs=200, learning_rate=0.1):\n    \"\"\"\n    Train model with controlled capacity to demonstrate overfitting.\n    \n    Args:\n        train_feats: Training features\n        train_labs: Training labels\n        val_feats: Validation features\n        val_labs: Validation labels\n        n_params_multiplier: Scale number of parameters (1 = normal, >1 = more capacity)\n        epochs: Training iterations\n        learning_rate: Learning rate\n        \n    Returns:\n        Training and validation histories\n    \"\"\"\n    # Task 6.1: Implement model with variable capacity\n    # YOUR CODE HERE\n    \n    n_features = train_feats.shape[1]\n    n_params = int(n_features * n_params_multiplier)\n    \n    # Add polynomial features to increase model capacity\n    if n_params_multiplier > 1:\n        # Square features for more expressive power\n        train_feats_expanded = torch.cat([train_feats, train_feats ** 2], dim=1)\n        val_feats_expanded = torch.cat([val_feats, val_feats ** 2], dim=1)\n    else:\n        train_feats_expanded = train_feats\n        val_feats_expanded = val_feats\n    \n    weights = torch.randn(train_feats_expanded.shape[1], requires_grad=True) * 0.1\n    bias = torch.zeros(1, requires_grad=True)\n    \n    optimizer = optim.Adam([weights, bias], lr=learning_rate)\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        # Training\n        train_preds = logistic_model(train_feats_expanded, weights, bias)\n        train_loss = binary_cross_entropy_loss(train_preds, train_labs)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        # Validation\n        with torch.no_grad():\n            val_preds = logistic_model(val_feats_expanded, weights, bias)\n            val_loss = binary_cross_entropy_loss(val_preds, val_labs)\n        \n        train_losses.append(train_loss.item())\n        val_losses.append(val_loss.item())\n    \n    return train_losses, val_losses\n\n# Compare different model capacities\nprint(\"Comparing model capacities:\\n\")\n\ncapacities = [1, 2, 5]  # 1x, 2x, 5x parameters\nplt.figure(figsize=(15, 10))\n\nfor idx, capacity in enumerate(capacities, 1):\n    print(f\"Training with {capacity}x capacity...\")\n    train_losses, val_losses = train_with_capacity_control(\n        train_feats, train_labs, val_feats, val_labs,\n        n_params_multiplier=capacity, epochs=200, learning_rate=0.05\n    )\n    \n    plt.subplot(2, 2, idx)\n    plt.plot(train_losses, label='Training', alpha=0.7)\n    plt.plot(val_losses, label='Validation', alpha=0.7)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(f'Model Capacity: {capacity}x')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    if capacity > 1 and val_losses[-1] > val_losses[len(val_losses)//2]:\n        plt.axvline(x=np.argmin(val_losses), color='r', linestyle='--', \n                   label='Best Val Loss', alpha=0.5)\n\nplt.subplot(2, 2, 4)\nplt.text(0.5, 0.5, 'Signs of Overfitting:\\n\\n'\n                   '1. Validation loss increases\\n'\n                   '   while training decreases\\n\\n'\n                   '2. Large gap between\\n'\n                   '   train and val loss\\n\\n'\n                   '3. Best validation occurs\\n'\n                   '   early in training',\n         ha='center', va='center', fontsize=12,\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(train_feats, train_labs, val_feats, val_labs,\n                               epochs=200, patience=10, learning_rate=0.1):\n    \"\"\"\n    Train with early stopping to prevent overfitting.\n    \n    Args:\n        train_feats: Training features\n        train_labs: Training labels\n        val_feats: Validation features\n        val_labs: Validation labels\n        epochs: Maximum training iterations\n        patience: Number of epochs to wait for improvement\n        learning_rate: Learning rate\n        \n    Returns:\n        Best model parameters and training history\n    \"\"\"\n    # Task 6.2: Implement early stopping\n    # YOUR CODE HERE\n    \n    n_features = train_feats.shape[1]\n    weights = torch.randn(n_features, requires_grad=True) * 0.1\n    bias = torch.zeros(1, requires_grad=True)\n    \n    optimizer = optim.Adam([weights, bias], lr=learning_rate)\n    \n    best_val_loss = float('inf')\n    best_weights = None\n    best_bias = None\n    patience_counter = 0\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        # Training\n        train_preds = logistic_model(train_feats, weights, bias)\n        train_loss = binary_cross_entropy_loss(train_preds, train_labs)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        # Validation\n        with torch.no_grad():\n            val_preds = logistic_model(val_feats, weights, bias)\n            val_loss = binary_cross_entropy_loss(val_preds, val_labs)\n        \n        train_losses.append(train_loss.item())\n        val_losses.append(val_loss.item())\n        \n        # Early stopping logic\n        if val_loss.item() < best_val_loss:\n            best_val_loss = val_loss.item()\n            best_weights = weights.clone().detach()\n            best_bias = bias.clone().detach()\n            patience_counter = 0\n            print(f\"Epoch {epoch+1}: New best validation loss: {best_val_loss:.4f}\")\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n            print(f\"Best validation loss: {best_val_loss:.4f}\")\n            break\n    \n    return best_weights, best_bias, train_losses, val_losses, epoch+1\n\n# Train with early stopping\nprint(\"Training with early stopping:\\n\")\nbest_w, best_b, train_l, val_l, stopped_epoch = train_with_early_stopping(\n    train_feats, train_labs, val_feats, val_labs,\n    epochs=200, patience=15, learning_rate=0.1\n)\n\n# Visualize early stopping\nplt.figure(figsize=(10, 6))\nplt.plot(train_l, label='Training', alpha=0.7)\nplt.plot(val_l, label='Validation', alpha=0.7)\nplt.axvline(x=stopped_epoch, color='r', linestyle='--', label='Early Stop', alpha=0.7)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Early Stopping in Action')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83c\udfaf Challenge Problems\n\n### Challenge 1: Multi-Class Gene Expression Classification\n\nExtend the binary classifier to predict gene expression levels in multiple categories (low, medium, high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(n_samples=1000, n_classes=3, seed=42):\n    \"\"\"\n    Generate synthetic multi-class gene expression data.\n    \n    Classes: 0=Low, 1=Medium, 2=High expression\n    \"\"\"\n    # Task: Implement multi-class data generation\n    # YOUR CODE HERE\n    \n    torch.manual_seed(seed)\n    \n    features_list = []\n    labels_list = []\n    \n    samples_per_class = n_samples // n_classes\n    \n    for class_id in range(n_classes):\n        # Each class has distinct feature distributions\n        class_features = torch.randn(samples_per_class, 5) + class_id * 2\n        class_labels = torch.full((samples_per_class,), class_id, dtype=torch.long)\n        \n        features_list.append(class_features)\n        labels_list.append(class_labels)\n    \n    features = torch.cat(features_list)\n    labels = torch.cat(labels_list)\n    \n    # Shuffle\n    indices = torch.randperm(n_samples)\n    return features[indices], labels[indices]\n\n# Generate multi-class data\nmulti_feats, multi_labels = generate_multiclass_data()\n\nprint(f\"Multi-class data:\")\nprint(f\"  Features: {multi_feats.shape}\")\nprint(f\"  Labels: {multi_labels.shape}\")\nprint(f\"  Classes: {multi_labels.unique()}\")\nprint(f\"\\nClass distribution:\")\nfor c in range(3):\n    print(f\"  Class {c}: {(multi_labels == c).sum().item()}\")\n\n# Hint: Use F.cross_entropy() loss and F.softmax() for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Learning Rate Scheduling\n\nImplement learning rate scheduling to improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(train_feats, train_labs, val_feats, val_labs,\n                            epochs=100, initial_lr=0.1, lr_decay=0.95):\n    \"\"\"\n    Train with exponentially decaying learning rate.\n    \n    Args:\n        train_feats: Training features\n        train_labs: Training labels  \n        val_feats: Validation features\n        val_labs: Validation labels\n        epochs: Training iterations\n        initial_lr: Starting learning rate\n        lr_decay: Multiplicative decay factor per epoch\n        \n    Returns:\n        Training history and learning rate schedule\n    \"\"\"\n    # Task: Implement learning rate scheduling\n    # YOUR CODE HERE\n    \n    n_features = train_feats.shape[1]\n    weights = torch.randn(n_features, requires_grad=True) * 0.1\n    bias = torch.zeros(1, requires_grad=True)\n    \n    optimizer = optim.SGD([weights, bias], lr=initial_lr)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n    \n    train_losses = []\n    val_losses = []\n    learning_rates = []\n    \n    for epoch in range(epochs):\n        # Training\n        train_preds = logistic_model(train_feats, weights, bias)\n        train_loss = binary_cross_entropy_loss(train_preds, train_labs)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        # Validation\n        with torch.no_grad():\n            val_preds = logistic_model(val_feats, weights, bias)\n            val_loss = binary_cross_entropy_loss(val_preds, val_labs)\n        \n        # Record metrics\n        train_losses.append(train_loss.item())\n        val_losses.append(val_loss.item())\n        learning_rates.append(optimizer.param_groups[0]['lr'])\n        \n        # Update learning rate\n        scheduler.step()\n        \n        if (epoch + 1) % 20 == 0:\n            print(f\"Epoch {epoch+1}: LR = {learning_rates[-1]:.6f}, \"\n                  f\"Train Loss = {train_loss.item():.4f}, \"\n                  f\"Val Loss = {val_loss.item():.4f}\")\n    \n    return train_losses, val_losses, learning_rates\n\n# Train with LR scheduling\nprint(\"Training with learning rate scheduling:\\n\")\ntrain_l, val_l, lrs = train_with_lr_schedule(\n    train_feats, train_labs, val_feats, val_labs,\n    epochs=100, initial_lr=0.5, lr_decay=0.97\n)\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(train_l, label='Training')\nax1.plot(val_l, label='Validation')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Loss with LR Scheduling')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(lrs)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Learning Rate')\nax2.set_title('Learning Rate Schedule')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \u2705 Self-Assessment\n\nBefore moving to Session 3, ensure you can:\n\n- [ ] Explain how autograd builds computation graphs\n- [ ] Implement gradient descent manually\n- [ ] Use PyTorch optimizers (SGD, Adam, RMSprop)\n- [ ] Build complete training loops with proper gradient handling\n- [ ] Split data into training and validation sets\n- [ ] Monitor training to detect overfitting\n- [ ] Implement early stopping\n- [ ] Compare different optimizers and learning rates\n- [ ] Use `torch.no_grad()` appropriately\n- [ ] Debug common gradient-related issues\n\n---\n\n## \ud83d\udcdd Additional Practice Ideas\n\n1. **Experiment with momentum**: Add momentum to SGD and compare convergence\n2. **Batch training**: Modify code to use mini-batches instead of full batch\n3. **Weight decay**: Add L2 regularization to prevent overfitting\n4. **Gradient clipping**: Implement gradient clipping for stable training\n5. **Custom loss functions**: Create domain-specific loss for genomics\n6. **Visualization**: Plot decision boundaries for 2D variant data\n7. **Real data**: Download variant data from ClinVar and train a real classifier\n\n---\n\n## \ud83d\ude80 Next Steps\n\nOnce you're comfortable with these exercises, move on to:\n- **Session 3**: Building Neural Networks with `nn.Module`\n- Try implementing batch training (process data in chunks)\n- Experiment with different architectures and hyperparameters\n\n---\n\n## \ud83d\udcda Key Takeaways\n\n### The Training Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize parameters\nparams = torch.randn(..., requires_grad=True)\n\n# 2. Create optimizer\noptimizer = optim.Adam([params], lr=0.1)\n\n# 3. Training loop\nfor epoch in range(epochs):\n    # Forward pass\n    predictions = model(inputs, params)\n    loss = loss_function(predictions, targets)\n    \n    # Backward pass\n    optimizer.zero_grad()  # Clear old gradients\n    loss.backward()         # Compute new gradients\n    optimizer.step()        # Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pitfalls to Avoid\n1. **Forgetting `zero_grad()`**: Gradients accumulate!\n2. **Wrong `requires_grad`**: Parameters must have `requires_grad=True`\n3. **Training without validation**: Always monitor validation loss\n4. **Ignoring overfitting**: Stop when validation loss increases\n5. **Bad learning rate**: Too high = unstable, too low = slow\n\n---\n\n## \ud83d\udcd6 Additional Resources\n\n- PyTorch Autograd Tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n- Understanding Optimizers: https://pytorch.org/docs/stable/optim.html\n- Loss Functions Guide: https://pytorch.org/docs/stable/nn.html#loss-functions\n- Gradient Descent Visualization: https://distill.pub/2017/momentum/\n\nGood luck with your practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }, 
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
